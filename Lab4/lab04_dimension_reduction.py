# -*- coding: utf-8 -*-
"""Lab04 - Dimension Reduction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_nqheeUQ0gRrVTHPQmj6vNVsVaE1yzFo
"""

#In this lab, the key goal is to try and do dimension reduction. To this end, we will use a dataset of our choise and play with dimension reduction techniques. We will focus on text dataset.
#We will follow the example mentioned in this path-breaking paper: http://wordvec.colorado.edu/papers/Landauer_Foltz_Laham_1998.pdf
import string
import re
import nltk
nltk.download('punkt')
from nltk import word_tokenize
from sklearn.preprocessing import MultiLabelBinarizer
from nltk.corpus import stopwords
nltk.download('stopwords')
from scipy.linalg import svd
import matplotlib.pyplot as plt
from mlxtend.data import iris_data
from mlxtend.plotting import scatterplotmatrix

#In the command below we are creating our sample dataset that we will use in this lab. This is a text dataset in the raw form.
Sentences = ["Human machine interface for ABC computer applications", "A survey of user opinion of computer system response time", "The EPS user interface management system", "System and human system engineering testing of EPS", "Relation of user perceived response time to error measurement", "The generation of random, binary, ordered trees", "The intersection graph of paths in trees", "Graph minors IV: Widths of trees and well-quasi-ordering", "Graph minors: A survey"]
print(Sentences)

#Let's do some pre-processing of the text above that we could later use in our analysis. In the code below we are iterating over each element in the list/array created above.
NewSentences = []
for OneSentence in Sentences:
    #print(OneSentence)
    NewSentences.append(OneSentence)

Sentences = NewSentences[:]
print(Sentences)

#Let's lowercase each word in the list above
NewSentences = []
for OneSentence in Sentences:
  OneSentence = OneSentence.lower()
  NewSentences.append(OneSentence)

Sentences = NewSentences[:]
print(Sentences)

#Next step is to remove stop words from text. This helps reduce the dimensions of the matrix that we create.
NewSentences = []
stopwords_dict = {word: 1 for word in stopwords.words("english")}
for OneSentence in Sentences:
  OneSentence = " ".join([word for word in OneSentence.split() if word not in stopwords_dict])
  NewSentences.append(OneSentence)

Sentences = NewSentences[:]
print(Sentences)

#Now, we remove punctuations from string
NewSentences = []
for word in Sentences:
    for character in word:
        if character in string.punctuation:
            word = word.replace(character,"")
    NewSentences.append(word)

#Sentences = NewSentences[:]
print(Sentences)

#The code above has a problem. Please look carefully and let me know where the problems are. We must fix the problems before moving ahead. Besides that, how will you ensure that the fixed text is used in your future runs.
NewSentences = []
for word in Sentences:
    for character in word:
        if character in string.punctuation:
            word = re.sub('[^a-zA-Z0-9]+\s*', ' ', word)
    NewSentences.append(word)

Sentences = NewSentences[:]
print(Sentences)

#A common step in natural language processing is tokenising the string (https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization).
Sentences = [word_tokenize(i) for i in Sentences]
print(Sentences)

#Create a term document matrix: https://en.wikipedia.org/wiki/Document-term_matrix
mlb = MultiLabelBinarizer()
TermDocumentMatrix = mlb.fit_transform(Sentences)
print(TermDocumentMatrix)

#Print the vocabulary: https://en.wikipedia.org/wiki/Vocabulary
print(mlb.classes_)

U, s, VT = svd(TermDocumentMatrix)
#print(U)
print(s)
#print(VT)

#Select only first 2 Eigen values
s = s[0:2]
print(s)

#Now, let's start reducing the dimension of the U and V matrices
#Remember, in matrix multiplication, the first matrix's number of columns must be equal to the number of rows in the second matrix.
#Print the matrix dimensions
print(U.shape)
print(VT.shape)
print(s.shape)
U_small = U[:,0:2]
VT_small = VT[:,0:2]
print(U_small)
print(VT_small)
ReducedU_small = U_small * s
ReducedVT_small = VT_small * s
print(ReducedU_small)
print(ReducedVT_small)

#Plot matrices
scatterplotmatrix(ReducedVT_small, figsize=(10, 8), names = mlb.classes_)
plt.tight_layout()
plt.show()

fig, ax = plt.subplots()
x = ReducedVT_small[:,0]
y = ReducedVT_small[:,1]
n = mlb.classes_
ax.scatter(y,x)
for i, txt in enumerate(n):
    ax.annotate(txt, (y[i], x[i]))